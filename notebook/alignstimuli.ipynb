{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb7f87b-f8d1-4f38-bfa6-b1e7820fd822",
   "metadata": {},
   "source": [
    "conda meg-analysis-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a10d493-98a9-4dfb-905a-63b95bdf5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- embeddings\n",
      "  - word2vec\n",
      "    - word-level\n",
      "      - 100d\n",
      "      - 300d\n",
      "    - char-level\n",
      "      - 100d\n",
      "      - 300d\n",
      "  - gpt\n",
      "    - word-level\n",
      "  - bert\n",
      "    - word-level\n",
      "    - char-level\n",
      "- frequency\n",
      "  - word-level\n",
      "  - char-level\n",
      "- quiz\n",
      "- scripts\n",
      "- time_align\n",
      "  - word-level\n",
      "  - char-level\n",
      "- syntactic_annotations\n",
      "  - dependency_parsing\n",
      "  - constituency_parsing\n",
      "  - part_of_speech\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_raw_dir = '../SMN4Lang_data/ds004078/derivatives/preprocessed_data/sub-01/MEG'\n",
    "base_ann_dir = '../SMN4Lang_data/ds004078/derivatives/annotations'\n",
    "base_sti_dir = '../SMN4Lang_data/ds004078/stimuli'\n",
    "\n",
    "import os\n",
    "\n",
    "def list_folders_recursive(path, indent=0):\n",
    "    folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "    for folder in folders:\n",
    "        print('  ' * indent + f'- {folder}')\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        # 再進一步列出下一層\n",
    "        list_folders_recursive(folder_path, indent + 1)\n",
    "\n",
    "# Example usage:\n",
    "list_folders_recursive(base_ann_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bf5c16e-e459-4b7f-b758-5b3b72f6e7db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PART OF SPEECH ---\n",
      "0\t我们\tPN\n",
      "1\t经常\tAD\n",
      "2\t会\tVV\n",
      "3\t说\tVV\n",
      "4\t教育\tNN\n",
      "5\t关系\tVV\n",
      "6\t千家万户\tNN\n",
      "7\t，\tPU\n",
      "8\t有关\tVV\n",
      "9\t教育\tNN\n",
      "10\t的\tDEC\n",
      "11\t讨论\tNN\n",
      "12\t总\tAD\n",
      "13\t能\tVV\n",
      "14\t引发\tVV\n",
      "15\t社会\tNN\n",
      "16\t关注\tVV\n",
      "17\t。\tPU\n",
      "\n",
      "0\t最近\tNT\n",
      "\n",
      "--- WORD FREQUENCY (word-level) ---\n",
      "[15.41563968 12.55887159 15.04833405 15.15246419 14.94689299 13.53182715\n",
      " 10.28530866 19.29483723 13.90786273 14.94689299 18.6445706  12.41426904\n",
      " 13.44249279 14.67172597 12.83701724 14.70894281 13.87025534 18.49681653\n",
      " 12.47815111 19.29483723]\n",
      "\n",
      "--- TIME ALIGN (word-level) ---\n",
      "[11.28 11.45 11.68 11.82 12.1  12.65 13.07 14.12 14.25 14.57 14.86 14.98\n",
      " 15.61 15.76 15.84 16.23 16.56 17.04 17.6  17.91]\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'wf'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'start', 'end', 'word'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "# base path\n",
    "base_ann_dir = '../SMN4Lang_data/ds004078/derivatives/annotations'\n",
    "\n",
    "### === PART OF SPEECH ===\n",
    "print('--- PART OF SPEECH ---')\n",
    "pos_path = os.path.join(base_ann_dir, 'syntactic_annotations/part_of_speech/story_1_pos.txt')\n",
    "with open(pos_path, 'r', encoding='utf-8') as f:\n",
    "    for i in range(20):\n",
    "        print(f.readline().strip())\n",
    "\n",
    "### === FREQUENCY (word-level) ===\n",
    "print('\\n--- WORD FREQUENCY (word-level) ---')\n",
    "freq_path = os.path.join(base_ann_dir, 'frequency/word-level/story_1_word_logfreq.mat')\n",
    "freqwordmat = sio.loadmat(freq_path)\n",
    "word_freq_values = freqwordmat['wf'].squeeze()\n",
    "print(word_freq_values[:20])\n",
    "\n",
    "### === TIME ALIGN (word-level) ===\n",
    "print('\\n--- TIME ALIGN (word-level) ---')\n",
    "time_path = os.path.join(base_ann_dir, 'time_align/word-level/story_1_word_time.mat')\n",
    "timewordmat = sio.loadmat(time_path)\n",
    "word_start_times = timewordmat['start'].squeeze()\n",
    "print(word_start_times[:20])\n",
    "\n",
    "\n",
    "'''\n",
    "### === DEPENDENCY PARSING ===\n",
    "print('\\n--- DEPENDENCY PARSING ---')\n",
    "dep_path = os.path.join(base_ann_dir, 'syntactic_annotations/dependency_parsing/story_1_dependency.conllx')\n",
    "with open(dep_path, 'r', encoding='utf-8') as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n",
    "\n",
    "### === CONSTITUENCY PARSING ===\n",
    "print('\\n--- CONSTITUENCY PARSING ---')\n",
    "const_path = os.path.join(base_ann_dir, 'syntactic_annotations/constituency_parsing/story_1_constituency.txt')\n",
    "with open(const_path, 'r', encoding='utf-8') as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n",
    "\n",
    "### === WORD2VEC embedding (word-level 300d) ===\n",
    "print('\\n--- WORD2VEC EMBEDDING (word-level, 300d) ---')\n",
    "w2v_path = os.path.join(base_ann_dir, 'embeddings/word2vec/word-level/300d/story_1_word_word2vec.mat')\n",
    "w2v_mat = sio.loadmat(w2v_path)\n",
    "w2v_values = w2v_mat['embedding']  # 通常叫 embedding\n",
    "print(w2v_values[:10])  # (n_words, embedding_dim)\n",
    "\n",
    "### === BERT embedding (word-level) ===\n",
    "print('\\n--- BERT EMBEDDING (word-level) ---')\n",
    "bert_path = os.path.join(base_ann_dir, 'embeddings/bert/word-level/story_1_word_bert_1-12_768.mat')\n",
    "bert_mat = sio.loadmat(bert_path)\n",
    "bert_values = bert_mat['embedding']\n",
    "print(bert_values[:10])  # (n_words, embedding_dim)\n",
    "'''\n",
    "\n",
    "print(freqwordmat.keys())\n",
    "print(timewordmat.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c3fe7-cdc8-472a-90c7-6867b1472c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b06fbad8-12a8-4fc7-917a-259b6c277606",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f31fcd2-5ca8-4de9-98e9-4fe035034fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word  pos  word_freq_log  word_onset_sec\n",
      "0   最近     NT      12.478151            0.59\n",
      "1   这      DT      15.077642            0.88\n",
      "2   段       M      12.593048            0.98\n",
      "3   时间     NN      14.674694            1.09\n",
      "4   ，      PU      19.294837            1.64\n",
      "5   与       P      16.051127            1.67\n",
      "6   儿童     NN      13.308856            1.87\n",
      "7   有关     VV      13.907863            2.17\n",
      "8   的     DEC      18.644571            2.49\n",
      "9   几      CD      10.071372            2.63\n",
      "10  款       M      12.132652            2.77\n",
      "11  新      JJ      15.704966            2.94\n",
      "12  产品     NN      14.661952            3.14\n",
      "13  和      CC      17.094431            3.56\n",
      "14  新      JJ      15.704966            3.72\n",
      "15  服务     NN      15.447769            3.88\n",
      "16  ，      PU      19.294837            4.37\n",
      "17  引发     VV      12.837017            4.49\n",
      "18  了      AS      17.074538            4.84\n",
      "19  不小     JJ      10.841070            4.96\n"
     ]
    }
   ],
   "source": [
    "#測試用需要的資料對齊不存檔\n",
    "# === Get word_text_list from timewordmat['word']\n",
    "words_raw = timewordmat['word']\n",
    "words_raw = np.squeeze(words_raw)\n",
    "\n",
    "# Decode if bytes\n",
    "if isinstance(words_raw[0], bytes):\n",
    "    word_text_list = [w.decode('utf-8') for w in words_raw]\n",
    "else:\n",
    "    word_text_list = list(words_raw)\n",
    "\n",
    "# === Read POS as list of (word, POS)\n",
    "pos_path = os.path.join(base_ann_dir, f'syntactic_annotations/part_of_speech/story_{story_id}_pos.txt')\n",
    "pos_word_list = []\n",
    "pos_tag_list = []\n",
    "with open(pos_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split()\n",
    "        if len(tokens) >= 3:\n",
    "            pos_word_list.append(tokens[1])\n",
    "            pos_tag_list.append(tokens[2])\n",
    "        else:\n",
    "            # 有些行可能是空行或缺欄位\n",
    "            continue\n",
    "\n",
    "# === 對齊 POS → 對齊到 word_text_list 裡有的詞\n",
    "# 常見做法：pos_word_list → lower casing、strip 處理\n",
    "# 最簡單做法：直接對齊順序！（你要確認你的 pos.txt 是否 \"只存詞性，不存 word\"，如果只存詞性就直接用 pos_list）\n",
    "\n",
    "# --- 如果 pos.txt 是只有 POS tag per line，則 pos_tag_list = pos_list\n",
    "\n",
    "# === Check length again after alignment\n",
    "n_words = len(word_text_list)\n",
    "if len(pos_tag_list) != n_words:\n",
    "    print(f'WARNING: POS length mismatch: {len(pos_tag_list)} vs {n_words}')\n",
    "    # 自動 truncate to shorter one → 避免報錯\n",
    "    min_len = min(len(pos_tag_list), n_words)\n",
    "    pos_tag_list = pos_tag_list[:min_len]\n",
    "    word_text_list = word_text_list[:min_len]\n",
    "    word_freq_values = word_freq_values[:min_len]\n",
    "    word_start_times = word_start_times[:min_len]\n",
    "\n",
    "# === Build DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'word': word_text_list,\n",
    "    'pos': pos_tag_list,\n",
    "    'word_freq_log': word_freq_values,\n",
    "    'word_onset_sec': word_start_times - 10.65\n",
    "})\n",
    "\n",
    "print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f81b148-3994-4698-b851-81444c6a18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing story 1 ===\n",
      "Saved to StimulusTables/story_1_stimulus_table.csv — n_words: 1044\n",
      "=== Processing story 2 ===\n",
      "Saved to StimulusTables/story_2_stimulus_table.csv — n_words: 939\n",
      "=== Processing story 3 ===\n",
      "Saved to StimulusTables/story_3_stimulus_table.csv — n_words: 902\n",
      "=== Processing story 4 ===\n",
      "Saved to StimulusTables/story_4_stimulus_table.csv — n_words: 908\n",
      "=== Processing story 5 ===\n",
      "Saved to StimulusTables/story_5_stimulus_table.csv — n_words: 871\n",
      "=== Processing story 6 ===\n",
      "Saved to StimulusTables/story_6_stimulus_table.csv — n_words: 885\n",
      "=== Processing story 7 ===\n",
      "Saved to StimulusTables/story_7_stimulus_table.csv — n_words: 840\n",
      "=== Processing story 8 ===\n",
      "Saved to StimulusTables/story_8_stimulus_table.csv — n_words: 781\n",
      "=== Processing story 9 ===\n",
      "Saved to StimulusTables/story_9_stimulus_table.csv — n_words: 735\n",
      "=== Processing story 10 ===\n",
      "Saved to StimulusTables/story_10_stimulus_table.csv — n_words: 710\n",
      "=== Processing story 11 ===\n",
      "Saved to StimulusTables/story_11_stimulus_table.csv — n_words: 1051\n",
      "=== Processing story 12 ===\n",
      "Saved to StimulusTables/story_12_stimulus_table.csv — n_words: 1001\n",
      "=== Processing story 13 ===\n",
      "Saved to StimulusTables/story_13_stimulus_table.csv — n_words: 831\n",
      "=== Processing story 14 ===\n",
      "Saved to StimulusTables/story_14_stimulus_table.csv — n_words: 917\n",
      "=== Processing story 15 ===\n",
      "Saved to StimulusTables/story_15_stimulus_table.csv — n_words: 864\n",
      "=== Processing story 16 ===\n",
      "Saved to StimulusTables/story_16_stimulus_table.csv — n_words: 809\n",
      "=== Processing story 17 ===\n",
      "Saved to StimulusTables/story_17_stimulus_table.csv — n_words: 894\n",
      "=== Processing story 18 ===\n",
      "Saved to StimulusTables/story_18_stimulus_table.csv — n_words: 828\n",
      "=== Processing story 19 ===\n",
      "Saved to StimulusTables/story_19_stimulus_table.csv — n_words: 844\n",
      "=== Processing story 20 ===\n",
      "Saved to StimulusTables/story_20_stimulus_table.csv — n_words: 692\n",
      "=== Processing story 21 ===\n",
      "Saved to StimulusTables/story_21_stimulus_table.csv — n_words: 1050\n",
      "=== Processing story 22 ===\n",
      "Saved to StimulusTables/story_22_stimulus_table.csv — n_words: 978\n",
      "=== Processing story 23 ===\n",
      "Saved to StimulusTables/story_23_stimulus_table.csv — n_words: 814\n",
      "=== Processing story 24 ===\n",
      "Saved to StimulusTables/story_24_stimulus_table.csv — n_words: 894\n",
      "=== Processing story 25 ===\n",
      "Saved to StimulusTables/story_25_stimulus_table.csv — n_words: 783\n",
      "=== Processing story 26 ===\n",
      "Saved to StimulusTables/story_26_stimulus_table.csv — n_words: 797\n",
      "=== Processing story 27 ===\n",
      "Saved to StimulusTables/story_27_stimulus_table.csv — n_words: 915\n",
      "=== Processing story 28 ===\n",
      "Saved to StimulusTables/story_28_stimulus_table.csv — n_words: 837\n",
      "=== Processing story 29 ===\n",
      "Saved to StimulusTables/story_29_stimulus_table.csv — n_words: 858\n",
      "=== Processing story 30 ===\n",
      "Saved to StimulusTables/story_30_stimulus_table.csv — n_words: 608\n",
      "=== Processing story 31 ===\n",
      "Saved to StimulusTables/story_31_stimulus_table.csv — n_words: 1076\n",
      "=== Processing story 32 ===\n",
      "Saved to StimulusTables/story_32_stimulus_table.csv — n_words: 923\n",
      "=== Processing story 33 ===\n",
      "Saved to StimulusTables/story_33_stimulus_table.csv — n_words: 850\n",
      "=== Processing story 34 ===\n",
      "Saved to StimulusTables/story_34_stimulus_table.csv — n_words: 969\n",
      "=== Processing story 35 ===\n",
      "Saved to StimulusTables/story_35_stimulus_table.csv — n_words: 965\n",
      "=== Processing story 36 ===\n",
      "Saved to StimulusTables/story_36_stimulus_table.csv — n_words: 876\n",
      "=== Processing story 37 ===\n",
      "Saved to StimulusTables/story_37_stimulus_table.csv — n_words: 887\n",
      "=== Processing story 38 ===\n",
      "Saved to StimulusTables/story_38_stimulus_table.csv — n_words: 827\n",
      "=== Processing story 39 ===\n",
      "Saved to StimulusTables/story_39_stimulus_table.csv — n_words: 771\n",
      "=== Processing story 40 ===\n",
      "Saved to StimulusTables/story_40_stimulus_table.csv — n_words: 739\n",
      "=== Processing story 41 ===\n",
      "Saved to StimulusTables/story_41_stimulus_table.csv — n_words: 1056\n",
      "=== Processing story 42 ===\n",
      "Saved to StimulusTables/story_42_stimulus_table.csv — n_words: 1016\n",
      "=== Processing story 43 ===\n",
      "Saved to StimulusTables/story_43_stimulus_table.csv — n_words: 861\n",
      "=== Processing story 44 ===\n",
      "Saved to StimulusTables/story_44_stimulus_table.csv — n_words: 836\n",
      "=== Processing story 45 ===\n",
      "Saved to StimulusTables/story_45_stimulus_table.csv — n_words: 780\n",
      "=== Processing story 46 ===\n",
      "Saved to StimulusTables/story_46_stimulus_table.csv — n_words: 840\n",
      "=== Processing story 47 ===\n",
      "Saved to StimulusTables/story_47_stimulus_table.csv — n_words: 826\n",
      "=== Processing story 48 ===\n",
      "Saved to StimulusTables/story_48_stimulus_table.csv — n_words: 857\n",
      "=== Processing story 49 ===\n",
      "Saved to StimulusTables/story_49_stimulus_table.csv — n_words: 788\n",
      "=== Processing story 50 ===\n",
      "Saved to StimulusTables/story_50_stimulus_table.csv — n_words: 839\n",
      "=== Processing story 51 ===\n",
      "Saved to StimulusTables/story_51_stimulus_table.csv — n_words: 1017\n",
      "=== Processing story 52 ===\n",
      "Saved to StimulusTables/story_52_stimulus_table.csv — n_words: 953\n",
      "=== Processing story 53 ===\n",
      "Saved to StimulusTables/story_53_stimulus_table.csv — n_words: 843\n",
      "=== Processing story 54 ===\n",
      "Saved to StimulusTables/story_54_stimulus_table.csv — n_words: 932\n",
      "=== Processing story 55 ===\n",
      "Saved to StimulusTables/story_55_stimulus_table.csv — n_words: 964\n",
      "=== Processing story 56 ===\n",
      "Saved to StimulusTables/story_56_stimulus_table.csv — n_words: 861\n",
      "=== Processing story 57 ===\n",
      "Saved to StimulusTables/story_57_stimulus_table.csv — n_words: 835\n",
      "=== Processing story 58 ===\n",
      "Saved to StimulusTables/story_58_stimulus_table.csv — n_words: 818\n",
      "=== Processing story 59 ===\n",
      "Saved to StimulusTables/story_59_stimulus_table.csv — n_words: 778\n",
      "=== Processing story 60 ===\n",
      "Saved to StimulusTables/story_60_stimulus_table.csv — n_words: 806\n"
     ]
    }
   ],
   "source": [
    "#此版會將需要的資料對齊存檔\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Base path ===\n",
    "base_ann_dir = '../SMN4Lang_data/ds004078/derivatives/annotations'\n",
    "save_csv_dir = 'StimulusTables'  # 改成你想存的地方\n",
    "os.makedirs(save_csv_dir, exist_ok=True)\n",
    "\n",
    "# === Loop over stories ===\n",
    "for story_id in range(1, 61):\n",
    "    print(f'=== Processing story {story_id} ===')\n",
    "\n",
    "    try:\n",
    "        ### === Read POS ===\n",
    "        pos_path = os.path.join(base_ann_dir, f'syntactic_annotations/part_of_speech/story_{story_id}_pos.txt')\n",
    "        pos_word_list = []\n",
    "        pos_tag_list = []\n",
    "        with open(pos_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                if len(tokens) >= 3:\n",
    "                    pos_word_list.append(tokens[1])  # word\n",
    "                    pos_tag_list.append(tokens[2])   # POS\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        ### === Read word freq ===\n",
    "        freq_path = os.path.join(base_ann_dir, f'frequency/word-level/story_{story_id}_word_logfreq.mat')\n",
    "        freqwordmat = sio.loadmat(freq_path)\n",
    "        word_freq_values = freqwordmat['wf'].squeeze()\n",
    "\n",
    "        ### === Read time align ===\n",
    "        time_path = os.path.join(base_ann_dir, f'time_align/word-level/story_{story_id}_word_time.mat')\n",
    "        timewordmat = sio.loadmat(time_path)\n",
    "        word_start_times = timewordmat['start'].squeeze()\n",
    "\n",
    "        # Get word list\n",
    "        words_raw = timewordmat['word']\n",
    "        words_raw = np.squeeze(words_raw)\n",
    "\n",
    "        # Decode if bytes\n",
    "        if isinstance(words_raw[0], bytes):\n",
    "            word_text_list = [w.decode('utf-8') for w in words_raw]\n",
    "        else:\n",
    "            word_text_list = list(words_raw)\n",
    "\n",
    "        ### === Check alignment\n",
    "        n_words = len(word_text_list)\n",
    "        if len(pos_tag_list) != n_words:\n",
    "            print(f'WARNING: Story {story_id}: POS length mismatch: {len(pos_tag_list)} vs {n_words}')\n",
    "            min_len = min(len(pos_tag_list), n_words)\n",
    "            pos_tag_list = pos_tag_list[:min_len]\n",
    "            word_text_list = word_text_list[:min_len]\n",
    "            word_freq_values = word_freq_values[:min_len]\n",
    "            word_start_times = word_start_times[:min_len]\n",
    "\n",
    "        ### === Build DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'word': word_text_list,\n",
    "            'pos': pos_tag_list,\n",
    "            'word_freq_log': word_freq_values,\n",
    "            'word_onset_sec': word_start_times - 10.65\n",
    "        })\n",
    "\n",
    "        ### === Save to CSV\n",
    "        save_path = os.path.join(save_csv_dir, f'story_{story_id}_stimulus_table.csv')\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f'Saved to {save_path} — n_words: {len(df)}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'ERROR processing story {story_id}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aba4500-a660-4484-a58b-bb6c03a362e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  pos  word_freq_log  word_onset_sec\n",
      "0   我们      PN      15.415640            0.63\n",
      "1   经常      AD      12.558872            0.80\n",
      "2   会       VV      15.048334            1.03\n",
      "3   说       VV      15.152464            1.17\n",
      "4   教育      NN      14.946893            1.45\n",
      "5   关系      VV      13.531827            2.00\n",
      "6   千家万户    NN      10.285309            2.42\n",
      "7   ，       PU      19.294837            3.47\n",
      "8   有关      VV      13.907863            3.60\n",
      "9   教育      NN      14.946893            3.92\n",
      "10  的      DEC      18.644571            4.21\n",
      "11  讨论      NN      12.414269            4.33\n",
      "12  总       AD      13.442493            4.96\n",
      "13  能       VV      14.671726            5.11\n",
      "14  引发      VV      12.837017            5.19\n",
      "15  社会      NN      14.708943            5.58\n",
      "16  关注      VV      13.870255            5.91\n",
      "17  。       PU      18.496817            6.39\n",
      "18  最近      NT      12.478151            6.95\n",
      "19  ，       PU      19.294837            7.26\n"
     ]
    }
   ],
   "source": [
    "#讀取存好的檔案\n",
    "import pandas as pd\n",
    "\n",
    "story_id = 1\n",
    "csv_path = f'StimulusTables/story_{story_id}_stimulus_table.csv'\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 看前幾筆確認\n",
    "print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e92ff-cd8f-491f-96c9-d3335e4b58d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
